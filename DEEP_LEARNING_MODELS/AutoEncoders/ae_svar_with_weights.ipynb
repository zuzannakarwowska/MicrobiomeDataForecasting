{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56c64a7a-08b5-459c-9bb3-c9c04ede4e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#base\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "import random\n",
    "#keras\n",
    "import keras.backend as K\n",
    "import tensorflow as tf \n",
    "#sklearn\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, MaxAbsScaler, RobustScaler, QuantileTransformer, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.stats.mstats import gmean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be402cc-b32f-4e1d-bbc7-6df4c761d089",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "sns.set_theme()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bfdc18d1-8b26-4da1-997c-5799fbcb86ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/MCB/zkarwowska/AE\n"
     ]
    }
   ],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "04cba48f-054b-438f-a13b-79d89f7946a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('rarefied_double_interpolated_feces_male_otu.csv', index_col = [0])\n",
    "x_train = df.iloc[:220]\n",
    "x_test = df.iloc[221:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4373a827-7c08-4614-99e4-f04f6b0d9fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_rare_bacteria(df):\n",
    "    \n",
    "    rare_bacteria_df = pd.DataFrame((df == 0).astype(int).sum(axis = 0))\n",
    "    rare_bacteria_col = rare_bacteria_df[rare_bacteria_df[0] > 250].index\n",
    "    df_filtered = df.drop(rare_bacteria_col, axis = 1)\n",
    "    \n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cb99f2fc-afb2-4be8-b2bd-4e3e816e8e6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = filter_rare_bacteria(df)\n",
    "#filtered_df.to_csv('improve_latent/filtered_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cce9be3-6da0-4e73-b991-10330f2a463d",
   "metadata": {},
   "source": [
    "### declare model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5f15269f-a23e-456a-a3d9-cbe302695fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#original model\n",
    "def ae_svar(normalized_train, normalized_test):\n",
    "    \n",
    "    l2_norm = 0\n",
    "    h_neurons = 256\n",
    "    l_neurons = 64\n",
    "    input_size = normalized_train.shape[1]\n",
    "\n",
    "    weights = tf.keras.Input(shape=(input_size,), name = 'weights_input')\n",
    "    weights_svar = tf.keras.Input(shape=(input_size,), name = 'weights_svar')\n",
    "    reconstruction_targets = tf.keras.Input(shape=(input_size,), name = 'reconstruction_targets')\n",
    "    svar_targets = tf.keras.Input(shape=(input_size,), name = 'svar_targets')\n",
    "\n",
    "    input_data_1 = tf.keras.Input(shape=(input_size,), name = 'input_data_1')\n",
    "    input_data_2 = tf.keras.Input(shape=(input_size,), name = 'input_data_2')\n",
    "    input_svar = tf.keras.Input(shape=(input_size,), name = 'input_svar')\n",
    "\n",
    "    first_layer = tf.keras.layers.Dense(h_neurons, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2_norm), name = 'first_layer')\n",
    "\n",
    "    encoded_1 = first_layer(input_data_1)\n",
    "    encoded_2 = first_layer(input_data_2)\n",
    "    encoded_svar = first_layer(input_svar)\n",
    "\n",
    "    #latent\n",
    "    second_layer = tf.keras.layers.Dense(l_neurons, kernel_regularizer=tf.keras.regularizers.L2(l2_norm), name = 'second_layer')\n",
    "\n",
    "    latent_1 = second_layer(encoded_1)\n",
    "    latent_2 = second_layer(encoded_2)\n",
    "    latent_svar = second_layer(encoded_svar)\n",
    "\n",
    "    input_decoder = tf.keras.layers.Input(l_neurons, name = 'input_decoder')\n",
    "    svar_layer_1 = tf.keras.layers.Dense(l_neurons, kernel_regularizer=tf.keras.regularizers.L1L2(l1=0.01, l2=0.01), name = 'svar_layer_1')\n",
    "    latent_svar_train = svar_layer_1(latent_1)\n",
    "    latent_svar_pred = svar_layer_1(latent_svar)\n",
    "    latent_svar_encoded = svar_layer_1(latent_2)\n",
    "\n",
    "    first_decoder = tf.keras.layers.Dense(h_neurons, activation='relu', kernel_regularizer=tf.keras.regularizers.L2(l2_norm), name = 'first_decoder')\n",
    "    decoder_1 = first_decoder(latent_1)\n",
    "    decoder_svar = first_decoder(latent_svar_train)\n",
    "    decoder = first_decoder(input_decoder)\n",
    "    decoder_svar_pred = first_decoder(latent_svar_pred)\n",
    "\n",
    "    output1 = tf.keras.layers.Dense(input_size, activation='linear', kernel_regularizer=tf.keras.regularizers.L2(l2_norm), name = 'output1')(decoder_1)\n",
    "    output_svar = tf.keras.layers.Dense(input_size, activation='linear', kernel_regularizer=tf.keras.regularizers.L2(l2_norm), name = 'output_svar')(decoder_svar)\n",
    "    output = tf.keras.layers.Dense(input_size, activation='linear', name = 'output')(decoder)\n",
    "    output_svar_pred = tf.keras.layers.Dense(input_size, activation='linear', kernel_regularizer=tf.keras.regularizers.L2(l2_norm), name = 'output_svar_pred')(decoder_svar_pred)\n",
    "\n",
    "\n",
    "    # Compile model\n",
    "    final_output1 = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.NONE)(output1,reconstruction_targets)\n",
    "    final_output_svar = tf.keras.losses.Huber(reduction=tf.keras.losses.Reduction.NONE)(output_svar, svar_targets)    \n",
    "    final_output1 = tf.keras.layers.Multiply()([final_output1, weights])\n",
    "    final_output_svar = tf.keras.layers.Multiply()([final_output_svar, weights_svar])  \n",
    "\n",
    "    svar_autoencoder = tf.keras.Model([input_data_1, weights, weights_svar, reconstruction_targets, svar_targets], [final_output1, final_output_svar]) \n",
    "\n",
    "    decoder_model = tf.keras.Model(input_decoder, output)\n",
    "    encoder_model = tf.keras.Model(input_data_2, [latent_2, latent_svar_encoded])\n",
    "\n",
    "\n",
    "    svar_autoencoder.compile(\n",
    "        loss = [tf.keras.losses.MeanAbsoluteError(),tf.keras.losses.MeanAbsoluteError()],\n",
    "        loss_weights = [1.0, 60.0],\n",
    "        optimizer='adam'\n",
    "    )\n",
    "    \n",
    "    weights = (x_train!=0).values*2\n",
    "    test_weights = np.ones((x_test.shape[0], x_test.shape[1]), dtype = int)\n",
    "\n",
    "\n",
    "    ae_result = svar_autoencoder.fit(\n",
    "        [normalized_train[:-1], weights[:-1], weights[1:], normalized_train[:-1], normalized_train[1:]],\n",
    "        [np.zeros_like(normalized_train[:-1]), np.zeros_like(normalized_train[1:])],\n",
    "        #[normalized_train[:-1], normalized_train[1:]],\n",
    "        epochs=50,\n",
    "        batch_size=16,\n",
    "        shuffle=False,\n",
    "        verbose = 0,\n",
    "        validation_data=([normalized_test[:-1], test_weights[:-1], test_weights[1:], normalized_test[:-1], normalized_test[1:]],\n",
    "        [normalized_test[:-1], normalized_test[1:]]),\n",
    "    )\n",
    "    \n",
    "    return ae_result, encoder_model, decoder_model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_ae",
   "language": "python",
   "name": "deep_ae"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
