{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eacc7b1-4f46-4e11-b2ae-eadc65d6aa2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import os\n",
    "import sys\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedKFold, KFold,RandomizedSearchCV\n",
    "\n",
    "\n",
    "import skbio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c681be81-a390-4ac3-a628-1708734e0b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "MODULE_PATH = os.path.abspath('/storage/zkarwowska/causality_analysis/VAR_MODELS/helper_functions/') # TODO load from CONFIG file\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "\n",
    "from ProcessingFunctions import MicrobiomeDataPreprocessing\n",
    "\n",
    "MODULE_PATH = os.path.abspath('/storage/pszczerbiak/microbiome_interactions_project') # TODO load from CONFIG file\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "    \n",
    "from utils.measures import calculate_spearman, calculate_nrmse, inter_dissimilarity\n",
    "\n",
    "MODULE_PATH = os.path.abspath('/storage/pszczerbiak/microbiome_interactions_project') # TODO load from CONFIG file\n",
    "if MODULE_PATH not in sys.path:\n",
    "    sys.path.append(MODULE_PATH)\n",
    "    \n",
    "from utils.transformers import CLRTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09c87664-121b-424b-b58d-b8161c1ba7db",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "sns.set_style('darkgrid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f621e9-bb97-4b32-9e12-f62b6413d853",
   "metadata": {},
   "source": [
    "#### READ DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2d4ec31-a8d7-4b7f-9a9c-c99cfe32f509",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_donorA = '/storage/zkarwowska/microbiome-interactions/datasets/processed/ready_datasets/male_rarefied_interpolated_feces.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8301a17-8886-4f56-93ae-0eabe1f207f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "savefile_name = '/storage/zkarwowska/causality_analysis/VAR_MODELS/REGRESSION_ANALYSIS_RESULTS/male_lag1/'\n",
    "lag = 'lag1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96ed17c-9c28-40f9-8af0-c075edd3c213",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(file_donorA, index_col = [0])\n",
    "df = df.sort_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4bf3c3f-d867-4ed0-9bce-5d14073db701",
   "metadata": {},
   "source": [
    "#### FILTER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aff956d-98e7-4aee-b763-92077f8ef1b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "processing = MicrobiomeDataPreprocessing()\n",
    "df_filtered = processing.filter_bacteria(df, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62f0d476-6e44-4043-8d62-2e7e0b8034ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e479d788-1d3a-46ee-94a2-345c00187955",
   "metadata": {},
   "source": [
    "#### TRANSFORM USING CLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aa71bb4-c363-444a-9cae-bd846e22f44a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clr_transformer = CLRTransformer(is_pseudo_global=True, axis=1)\n",
    "df_filtered_clr = clr_transformer.fit_transform(df_filtered)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca6db08a-6568-4471-b7dd-dcf6171f0a27",
   "metadata": {},
   "source": [
    "#### MAKE SUPERVISED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6bf545-5c88-4062-80b7-5d7fade502f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_filtered_supervised = processing.make_supervised(df_filtered, 1)\n",
    "df_filtered_clr_supervised = processing.make_supervised(df_filtered_clr, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbecb5dd-4718-49a0-8824-19ca8cacbe81",
   "metadata": {},
   "source": [
    "#### SPLIT TO TRAIN AND TEST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea5c5dee-42cc-47d0-9f20-9e3a434e90c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test  = processing.train_test_split(df_filtered_supervised, 0.2)\n",
    "train_clr, test_clr  = processing.train_test_split(df_filtered_clr_supervised, 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b364828-9e70-4b21-b530-64f4cfabc52d",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features = df_filtered.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f9481f-4c3a-4b86-aba8-625f0f527815",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2aca531-2f37-49d5-8259-d1c540628b02",
   "metadata": {},
   "source": [
    "model = XGBRegressor(objective='reg:squarederror')\n",
    "cv = RepeatedKFold(n_splits=2, n_repeats=2, random_state=1)\n",
    "n_scores = cross_val_score(model, X_train, y_train, scoring='neg_mean_absolute_error', cv=cv, n_jobs=-1, error_score='raise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7762202f-322a-425a-873e-bd9d6a21f4ac",
   "metadata": {},
   "source": [
    "#### OLS XgBoost VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "81b0a2be-78e8-41a2-af52-b4b84309583d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ols_xgboost_var(train, i, test, train_clr, test_clr):\n",
    "    \n",
    "    y_train = train.filter(like = 'lag0')\n",
    "    y_train = y_train.iloc[:, i] \n",
    "\n",
    "    y_test = test.filter(like = 'lag0')\n",
    "    y_test = y_test.iloc[:, i]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = train_clr.drop(train_clr.filter(like='lag0').columns, axis=1)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    X_test = test_clr.drop(test_clr.filter(like='lag0').columns, axis=1)  \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    params = {\n",
    "            'n_estimators' : [25, 50, 100],\n",
    "            'reg_alpha' : [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "            'reg_lambda' : [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "            }\n",
    "\n",
    "\n",
    "    xgb = XGBRegressor(objective='reg:squarederror', n_jobs=15, booster = 'gblinear')\n",
    "    folds = 5\n",
    "    param_comb = 20\n",
    "\n",
    "    kfld = KFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "    random_search = RandomizedSearchCV(xgb,\n",
    "                                       param_distributions=params,\n",
    "                                       n_iter=param_comb,\n",
    "                                       scoring='r2',\n",
    "                                       n_jobs=15,\n",
    "                                       cv=kfld.split(X_train_scaled,y_train), verbose=0,\n",
    "                                       random_state=1001 \n",
    "                                      )\n",
    "\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    best_xgb = XGBRegressor(objective='reg:squarederror', n_jobs=1, **random_search.best_params_)\n",
    "    best_xgb.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    prediction = best_xgb.predict(X_test_scaled)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d3bb02c-0551-41ce-bf16-6083d11e29f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "ols_predictions = pd.DataFrame()\n",
    "for i in range(0, n_features):\n",
    "    \n",
    "    pred = ols_xgboost_var(train, i, test, train_clr, test_clr)\n",
    "    ols_predictions[i] = pred\n",
    "    \n",
    "ols_predictions.to_csv(savefile_name + 'xgboost_linear_' + lag + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686131c7-56c0-438f-ab2c-a5dcc8d81d10",
   "metadata": {},
   "source": [
    "#### Poisson XgBoost VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e812f4e-e9ac-44dc-8549-96dfd85613b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poisson_xgboost_var(train, i, test, train_clr, test_clr):\n",
    "    \n",
    "    y_train = train.filter(like = 'lag0')\n",
    "    y_train = y_train.iloc[:, i] \n",
    "    #y_train = np.exp(y_train).multiply(clr_transformer.gmean_[:y_train.shape[0]])\n",
    "\n",
    "    y_test = test.filter(like = 'lag0')\n",
    "    y_test = y_test.iloc[:, i]\n",
    "    #y_test = np.exp(y_test).multiply(clr_transformer.gmean_[-y_test.shape[0]:])\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = train_clr.drop(train_clr.filter(like='lag0').columns, axis=1)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    X_test = test_clr.drop(test_clr.filter(like='lag0').columns, axis=1)  \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    params = {\n",
    "            'n_estimators' : [25, 50, 100],\n",
    "            'reg_alpha' : [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "            'reg_lambda' : [1e-1, 1e-2, 1e-3, 1e-4]\n",
    "            }\n",
    "\n",
    "\n",
    "    xgb = XGBRegressor(objective='count:poisson', n_jobs=15, booster = 'gblinear')\n",
    "    folds = 5\n",
    "    param_comb = 30\n",
    "\n",
    "    kfld = KFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "    random_search = RandomizedSearchCV(xgb,\n",
    "                                       param_distributions=params,\n",
    "                                       n_iter=param_comb,\n",
    "                                       scoring='r2',\n",
    "                                       n_jobs=15,\n",
    "                                       cv=kfld.split(X_train_scaled,y_train), verbose=0,\n",
    "                                       random_state=1001 \n",
    "                                      )\n",
    "\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    best_xgb = XGBRegressor(objective='count:poisson', n_jobs=1, **random_search.best_params_)\n",
    "    best_xgb.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    prediction = best_xgb.predict(X_test_scaled)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a46f90-0df9-4c6c-b447-0b1266acd7dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "poisson_predictions = pd.DataFrame()\n",
    "for i in range(0, n_features):\n",
    "    \n",
    "    pred = poisson_xgboost_var(train, i, test, train_clr, test_clr)\n",
    "    poisson_predictions[i] = pred\n",
    "    \n",
    "poisson_predictions.to_csv(savefile_name + 'xgboost_poisson_' + lag + '.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "895474a9-d83b-4299-9707-a9d8dc54b528",
   "metadata": {},
   "source": [
    "#### Tweedie XgBoost VAR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd11237b-0589-4622-84dd-68b5c371d3af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tweedie_xgboost_var(train, i, test, train_clr, test_clr):\n",
    "    \n",
    "    y_train = train.filter(like = 'lag0')\n",
    "    y_train = y_train.iloc[:, i] \n",
    "\n",
    "    y_test = test.filter(like = 'lag0')\n",
    "    y_test = y_test.iloc[:, i]\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "\n",
    "    X_train = train_clr.drop(train_clr.filter(like='lag0').columns, axis=1)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "\n",
    "    X_test = test_clr.drop(test_clr.filter(like='lag0').columns, axis=1)  \n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "\n",
    "\n",
    "    params = \n",
    "    {\n",
    "        'n_estimators' : [10, 20, 50, 100, 150],\n",
    "        'reg_alpha' : [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "        'reg_lambda' : [1e-1, 1e-2, 1e-3, 1e-4],\n",
    "        'tweedie_variance_power' : [0, 1, 1.5, 2, 3]\n",
    "    }\n",
    "\n",
    "\n",
    "    xgb = XGBRegressor(objective='reg:tweedie', n_jobs=15, booster = 'gblinear')\n",
    "    \n",
    "    folds = 5\n",
    "    param_comb = 20\n",
    "    kfld = KFold(n_splits=folds, shuffle = True, random_state = 1001)\n",
    "    random_search = RandomizedSearchCV(xgb,\n",
    "                                       param_distributions=params,\n",
    "                                       n_iter=param_comb,\n",
    "                                       scoring='r2',\n",
    "                                       n_jobs=15,\n",
    "                                       cv=kfld.split(X_train_scaled,y_train), \n",
    "                                       verbose=0,\n",
    "                                       random_state=1001 \n",
    "                                      )\n",
    "\n",
    "    random_search.fit(X_train_scaled, y_train)\n",
    "    best_xgb = XGBRegressor(objective='reg:tweedie', n_jobs=15, **random_search.best_params_)\n",
    "    best_xgb.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    \n",
    "    prediction = best_xgb.predict(X_test_scaled)\n",
    "    \n",
    "    return prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f184e69-d399-4308-99b0-994b0630bbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweedie_predictions = pd.DataFrame()\n",
    "for i in range(0, n_features):\n",
    "    pred = tweedie_xgboost_var(train, i, test, train_clr, test_clr)\n",
    "    tweedie_predictions[i] = pred\n",
    "tweedie_predictions.to_csv(savefile_name + 'xgboost_tweedie_' + lag + '.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "basic_env",
   "language": "python",
   "name": "basic_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
